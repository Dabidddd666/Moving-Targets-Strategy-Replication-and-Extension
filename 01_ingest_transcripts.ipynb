{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 1: Ingest and Normalize Transcripts\n",
        "\n",
        "This notebook:\n",
        "1. Fetches earnings call transcripts from Financial Modeling Prep API\n",
        "2. Cleans and normalizes the text\n",
        "3. Separates presentation vs Q&A sections (if available)\n",
        "4. Outputs standardized parquet file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key: wSb1mJ4mrG...\n",
            "Date range: 2010-2024\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import re\n",
        "from urllib.request import urlopen\n",
        "import certifi\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock, Semaphore\n",
        "import requests\n",
        "from collections import deque\n",
        "\n",
        "# Load config\n",
        "BASE_DIR = Path('/Users/david/Desktop/MATH-GA 2707/Moving Target')\n",
        "CONFIG_DIR = BASE_DIR / 'configs'\n",
        "INTERMEDIATE_DIR = BASE_DIR / 'data' / 'intermediate'\n",
        "\n",
        "with open(CONFIG_DIR / 'base.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Convert string paths back to Path objects\n",
        "for key in config['data']:\n",
        "    config['data'][key] = Path(config['data'][key])\n",
        "\n",
        "API_KEY = config['api']['fmp_api_key']\n",
        "BASE_URL = config['api']['base_url']\n",
        "START_YEAR = config['dates']['start_year']\n",
        "END_YEAR = config['dates']['end_year']\n",
        "\n",
        "print(f\"API Key: {API_KEY[:10]}...\")\n",
        "print(f\"Date range: {START_YEAR}-{END_YEAR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2611 tickers from Russell 3000\n"
          ]
        }
      ],
      "source": [
        "# Load Russell 3000 tickers\n",
        "with open(INTERMEDIATE_DIR / 'russell_3000_tickers.json', 'r') as f:\n",
        "    tickers = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(tickers)} tickers from Russell 3000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "def get_jsonparsed_data(url, max_retries=3):\n",
        "    \"\"\"Fetch and parse JSON from URL with retries\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Use requests instead of urlopen for better performance\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(0.5 * (attempt + 1))  # Exponential backoff\n",
        "                continue\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean transcript text\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove excessive whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Remove common boilerplate headers\n",
        "    boilerplate_patterns = [\n",
        "        r'Operator:.*?Please go ahead',\n",
        "        r'This call is being recorded',\n",
        "        r'Forward-looking statements.*?SEC today',\n",
        "    ]\n",
        "    for pattern in boilerplate_patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
        "    \n",
        "    # Normalize whitespace again\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def separate_sections(text):\n",
        "    \"\"\"Attempt to separate presentation from Q&A\"\"\"\n",
        "    # Common Q&A markers\n",
        "    qa_markers = [\n",
        "        r'Question-and-Answer Session',\n",
        "        r'Q&A Session',\n",
        "        r'Questions and Answers',\n",
        "        r'Operator:.*?questions',\n",
        "    ]\n",
        "    \n",
        "    presentation = text\n",
        "    qa = \"\"\n",
        "    \n",
        "    for marker in qa_markers:\n",
        "        match = re.search(marker, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            split_pos = match.start()\n",
        "            presentation = text[:split_pos].strip()\n",
        "            qa = text[split_pos:].strip()\n",
        "            break\n",
        "    \n",
        "    return presentation, qa\n",
        "\n",
        "print(\"Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching transcripts for ALL 2611 tickers...\n",
            "Date range: 2010 to 2024 (15 years)\n",
            "Total API requests: 156,660\n",
            "Using 40 parallel workers\n",
            "Estimated time: ~52.2 minutes\n",
            "Rate limit: 3000 calls/minute\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:   3%|▎         | 5009/156660 [01:10<2:16:43, 18.49it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 3391 transcripts after 5,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:   7%|▋         | 10354/156660 [03:03<26:28, 92.10it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 6712 transcripts after 10,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  10%|▉         | 14996/156660 [04:17<15:56, 148.09it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 9879 transcripts after 15,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  13%|█▎        | 19985/156660 [06:20<20:36, 110.57it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 13124 transcripts after 20,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  17%|█▋        | 25916/156660 [08:10<19:12, 113.45it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 16322 transcripts after 25,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  19%|█▉        | 29994/156660 [09:25<17:20, 121.71it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 19438 transcripts after 30,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  22%|██▏       | 35000/156660 [11:39<15:06:47,  2.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 22963 transcripts after 35,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  26%|██▌       | 40001/156660 [13:35<23:57:37,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 26403 transcripts after 40,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  29%|██▊       | 45000/156660 [15:13<37:03:17,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 29638 transcripts after 45,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  32%|███▏      | 50000/156660 [17:13<23:50:20,  1.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 32997 transcripts after 50,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  36%|███▋      | 56902/156660 [19:01<31:08, 53.38it/s]   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 36367 transcripts after 55,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  38%|███▊      | 60001/156660 [21:00<37:32:12,  1.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 39515 transcripts after 60,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  41%|████▏     | 65000/156660 [23:10<23:42:28,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 43073 transcripts after 65,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  45%|████▌     | 70602/156660 [25:42<2:00:02, 11.95it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 46569 transcripts after 70,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  48%|████▊     | 75229/156660 [29:27<11:40:41,  1.94it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 49963 transcripts after 75,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  52%|█████▏    | 81402/156660 [34:07<1:13:27, 17.07it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 53352 transcripts after 80,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  54%|█████▍    | 85000/156660 [38:30<1:41:33, 11.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 56654 transcripts after 85,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  57%|█████▋    | 90000/156660 [46:24<4:53:29,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 60098 transcripts after 90,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  61%|██████    | 95000/156660 [58:31<32:59:02,  1.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 63509 transcripts after 95,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  64%|██████▍   | 100000/156660 [1:05:20<7:57:56,  1.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 66746 transcripts after 100,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  67%|██████▋   | 105000/156660 [1:19:30<19:52:06,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 70029 transcripts after 105,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  70%|███████   | 110001/156660 [1:30:49<24:07:22,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Checkpoint: 73348 transcripts after 110,000 requests\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching transcripts:  77%|███████▋  | 119999/156660 [1:57:14<35:49, 17.06it/s]   \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m                 df_temp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(transcripts_list)\n\u001b[1;32m    141\u001b[0m                 checkpoint_file \u001b[38;5;241m=\u001b[39m INTERMEDIATE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscripts_checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompleted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 142\u001b[0m                 \u001b[43mdf_temp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(transcripts_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m transcripts after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompleted\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requests\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 480\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parquet.py:190\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[0;32m--> 190\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    193\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/table.pxi:4795\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyarrow/pandas_compat.py:643\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(nthreads) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(columns_to_convert, convert_fields):\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _can_definitely_zero_copy(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m):\n\u001b[1;32m    644\u001b[0m             arrays\u001b[38;5;241m.\u001b[39mappend(convert_column(c, f))\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:829\u001b[0m, in \u001b[0;36mSeries.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    791\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;124;03m    Return Series as ndarray or ndarray-like depending on the dtype.\u001b[39;00m\n\u001b[1;32m    793\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m           '2013-01-03T05:00:00.000000000'], dtype='datetime64[ns]')\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/managers.py:2002\u001b[0m, in \u001b[0;36mSingleBlockManager.external_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexternal_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The array that Series.values returns\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2002\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/blocks.py:251\u001b[0m, in \u001b[0;36mBlock.external_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexternal_values\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexternal_values\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2827\u001b[0m, in \u001b[0;36mexternal_values\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   2822\u001b[0m             values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[0;32m-> 2827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexternal_values\u001b[39m(values: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2829\u001b[0m \u001b[38;5;124;03m    The array that Series.values returns (public attribute).\u001b[39;00m\n\u001b[1;32m   2830\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2834\u001b[0m \u001b[38;5;124;03m    proper extension array).\u001b[39;00m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (PeriodArray, IntervalArray)):\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Fetch transcripts for ALL tickers from start year to end year\n",
        "# Using parallel processing to speed up (3000 calls/minute limit)\n",
        "\n",
        "# Rate limiting: 3000 calls/minute = 50 calls/second\n",
        "# Use 40 concurrent workers to stay safely under limit\n",
        "MAX_WORKERS = 40\n",
        "CALLS_PER_MINUTE = 3000\n",
        "CALLS_PER_SECOND = CALLS_PER_MINUTE / 60  # ~50 calls/second\n",
        "\n",
        "# Rate limiter: track API call timestamps\n",
        "call_times = deque()\n",
        "rate_lock = Lock()\n",
        "\n",
        "def rate_limited_call():\n",
        "    \"\"\"Ensure we don't exceed 3000 calls per minute\"\"\"\n",
        "    with rate_lock:\n",
        "        now = time.time()\n",
        "        # Remove calls older than 1 minute\n",
        "        while call_times and call_times[0] < now - 60:\n",
        "            call_times.popleft()\n",
        "        \n",
        "        # If we're at the limit, wait\n",
        "        if len(call_times) >= CALLS_PER_MINUTE:\n",
        "            sleep_time = 60 - (now - call_times[0]) + 0.1\n",
        "            if sleep_time > 0:\n",
        "                time.sleep(sleep_time)\n",
        "                # Clean up again after waiting\n",
        "                now = time.time()\n",
        "                while call_times and call_times[0] < now - 60:\n",
        "                    call_times.popleft()\n",
        "        \n",
        "        # Record this call\n",
        "        call_times.append(time.time())\n",
        "\n",
        "# Thread-safe lists\n",
        "transcripts_list = []\n",
        "list_lock = Lock()\n",
        "failed_requests = []\n",
        "failed_lock = Lock()\n",
        "\n",
        "# Generate all URL requests\n",
        "def generate_requests():\n",
        "    \"\"\"Generate all API request URLs\"\"\"\n",
        "    requests_list = []\n",
        "    for ticker in tickers:\n",
        "        for year in range(START_YEAR, END_YEAR + 1):\n",
        "            for quarter in [1, 2, 3, 4]:\n",
        "                url = f\"{BASE_URL}/earning-call-transcript?symbol={ticker}&year={year}&quarter={quarter}&apikey={API_KEY}\"\n",
        "                requests_list.append({\n",
        "                    'url': url,\n",
        "                    'ticker': ticker,\n",
        "                    'year': year,\n",
        "                    'quarter': quarter\n",
        "                })\n",
        "    return requests_list\n",
        "\n",
        "def fetch_single_transcript(request_info):\n",
        "    \"\"\"Fetch a single transcript and return processed data\"\"\"\n",
        "    # Rate limit before making the call\n",
        "    rate_limited_call()\n",
        "    \n",
        "    url = request_info['url']\n",
        "    ticker = request_info['ticker']\n",
        "    year = request_info['year']\n",
        "    quarter = request_info['quarter']\n",
        "    \n",
        "    try:\n",
        "        data = get_jsonparsed_data(url)\n",
        "        \n",
        "        if data and len(data) > 0:\n",
        "            results = []\n",
        "            for item in data:\n",
        "                transcript_raw = item.get('content', '')\n",
        "                if transcript_raw:\n",
        "                    transcript_clean = clean_text(transcript_raw)\n",
        "                    text_presentation, text_qa = separate_sections(transcript_clean)\n",
        "                    \n",
        "                    results.append({\n",
        "                        'ticker': ticker,\n",
        "                        'symbol': item.get('symbol', ticker),\n",
        "                        'year': item.get('year', year),\n",
        "                        'quarter': item.get('period', f'Q{quarter}').replace('Q', ''),\n",
        "                        'date': item.get('date', ''),\n",
        "                        'text_raw': transcript_raw,\n",
        "                        'text_presentation': text_presentation,\n",
        "                        'text_qa': text_qa,\n",
        "                        'text_full': transcript_clean,\n",
        "                        'source_vendor': 'FMP',\n",
        "                        'call_id': f\"{ticker}_{year}_Q{quarter}\"\n",
        "                    })\n",
        "            return results\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        with failed_lock:\n",
        "            failed_requests.append({\n",
        "                'ticker': ticker,\n",
        "                'year': year,\n",
        "                'quarter': quarter,\n",
        "                'error': str(e)\n",
        "            })\n",
        "        return []\n",
        "\n",
        "# Generate all requests\n",
        "all_requests = generate_requests()\n",
        "total_requests = len(all_requests)\n",
        "\n",
        "print(f\"Fetching transcripts for ALL {len(tickers)} tickers...\")\n",
        "print(f\"Date range: {START_YEAR} to {END_YEAR} ({END_YEAR - START_YEAR + 1} years)\")\n",
        "print(f\"Total API requests: {total_requests:,}\")\n",
        "print(f\"Using {MAX_WORKERS} parallel workers\")\n",
        "print(f\"Estimated time: ~{total_requests / CALLS_PER_MINUTE:.1f} minutes\")\n",
        "print(f\"Rate limit: {CALLS_PER_MINUTE} calls/minute\\n\")\n",
        "\n",
        "# Process requests in parallel with rate limiting\n",
        "transcripts_list = []\n",
        "checkpoint_interval = 5000  # Save checkpoint every 5000 requests\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    # Submit all tasks\n",
        "    future_to_request = {\n",
        "        executor.submit(fetch_single_transcript, req): req \n",
        "        for req in all_requests\n",
        "    }\n",
        "    \n",
        "    # Process completed tasks with progress bar\n",
        "    completed = 0\n",
        "    for future in tqdm(as_completed(future_to_request), total=total_requests, desc=\"Fetching transcripts\"):\n",
        "        try:\n",
        "            results = future.result()\n",
        "            if results:\n",
        "                with list_lock:\n",
        "                    transcripts_list.extend(results)\n",
        "            \n",
        "            completed += 1\n",
        "            \n",
        "            # Save checkpoint periodically\n",
        "            if completed % checkpoint_interval == 0:\n",
        "                with list_lock:\n",
        "                    if transcripts_list:\n",
        "                        df_temp = pd.DataFrame(transcripts_list)\n",
        "                        checkpoint_file = INTERMEDIATE_DIR / f'transcripts_checkpoint_{completed}.parquet'\n",
        "                        df_temp.to_parquet(checkpoint_file, index=False, engine='pyarrow')\n",
        "                        print(f\"\\n  Checkpoint: {len(transcripts_list)} transcripts after {completed:,} requests\")\n",
        "        except Exception as e:\n",
        "            req = future_to_request[future]\n",
        "            with failed_lock:\n",
        "                failed_requests.append({\n",
        "                    'ticker': req['ticker'],\n",
        "                    'year': req['year'],\n",
        "                    'quarter': req['quarter'],\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Fetching complete!\")\n",
        "print(f\"  Total transcripts fetched: {len(transcripts_list)}\")\n",
        "print(f\"  Total requests processed: {total_requests:,}\")\n",
        "if failed_requests:\n",
        "    failed_tickers = set([r['ticker'] for r in failed_requests])\n",
        "    print(f\"  Failed requests: {len(failed_requests)}\")\n",
        "    print(f\"  Tickers with errors: {len(failed_tickers)}\")\n",
        "    if len(failed_tickers) <= 20:\n",
        "        print(f\"  Failed tickers: {sorted(failed_tickers)}\")\n",
        "    else:\n",
        "        print(f\"  Failed tickers (first 20): {sorted(list(failed_tickers))[:20]}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recovery: Load Latest Checkpoint and Resume\n",
        "\n",
        "If the process was interrupted, run this cell to recover your data and resume from where it left off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found latest checkpoint: /Users/david/Desktop/MATH-GA 2707/Moving Target/data/intermediate/transcripts_checkpoint_95000.parquet\n",
            "Checkpoint number: 95,000 requests\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# RECOVERY: Load latest checkpoint and identify what's already been processed\n",
        "import glob\n",
        "\n",
        "# Find all checkpoint files\n",
        "checkpoint_files = sorted(glob.glob(str(INTERMEDIATE_DIR / 'transcripts_checkpoint_*.parquet')))\n",
        "\n",
        "if checkpoint_files:\n",
        "    # Load the latest checkpoint\n",
        "    latest_checkpoint = checkpoint_files[-1]\n",
        "    print(f\"Found latest checkpoint: {latest_checkpoint}\")\n",
        "    \n",
        "    # Extract checkpoint number\n",
        "    checkpoint_num = int(latest_checkpoint.split('_')[-1].replace('.parquet', ''))\n",
        "    print(f\"Checkpoint number: {checkpoint_num:,} requests\")\n",
        "    \n",
        "    # Load the checkpoint data\n",
        "    df_checkpoint = pd.read_parquet(latest_checkpoint)\n",
        "    print(f\"\\nLoaded checkpoint with {len(df_checkpoint):,} transcripts\")\n",
        "    print(f\"Columns in checkpoint: {list(df_checkpoint.columns)}\")\n",
        "    print(f\"Unique tickers in checkpoint: {df_checkpoint['ticker'].nunique()}\")\n",
        "    \n",
        "    # Handle call_date column - create it from 'date' if needed\n",
        "    if 'call_date' not in df_checkpoint.columns:\n",
        "        if 'date' in df_checkpoint.columns:\n",
        "            print(\"  Creating 'call_date' from 'date' column...\")\n",
        "            df_checkpoint['call_date'] = pd.to_datetime(df_checkpoint['date'], errors='coerce')\n",
        "        else:\n",
        "            print(\"  Warning: No 'date' or 'call_date' column found. Creating placeholder dates.\")\n",
        "            df_checkpoint['call_date'] = pd.NaT\n",
        "    \n",
        "    # Show date range if available\n",
        "    if 'call_date' in df_checkpoint.columns and df_checkpoint['call_date'].notna().any():\n",
        "        print(f\"Date range: {df_checkpoint['call_date'].min()} to {df_checkpoint['call_date'].max()}\")\n",
        "    else:\n",
        "        print(\"Date range: Not available\")\n",
        "    \n",
        "    # Create a set of already processed (ticker, year, quarter) combinations\n",
        "    # Handle year and quarter columns - they might be named differently\n",
        "    if 'year' not in df_checkpoint.columns:\n",
        "        if 'fiscal_year' in df_checkpoint.columns:\n",
        "            df_checkpoint['year'] = df_checkpoint['fiscal_year']\n",
        "        elif 'fyearq' in df_checkpoint.columns:\n",
        "            df_checkpoint['year'] = df_checkpoint['fyearq']\n",
        "        else:\n",
        "            print(\"  Warning: No year column found. Cannot track processed combinations.\")\n",
        "            df_checkpoint['year'] = None\n",
        "    \n",
        "    if 'quarter' not in df_checkpoint.columns:\n",
        "        if 'fiscal_quarter' in df_checkpoint.columns:\n",
        "            df_checkpoint['quarter'] = df_checkpoint['fiscal_quarter']\n",
        "        elif 'fqtr' in df_checkpoint.columns:\n",
        "            df_checkpoint['quarter'] = df_checkpoint['fqtr']\n",
        "        else:\n",
        "            print(\"  Warning: No quarter column found. Cannot track processed combinations.\")\n",
        "            df_checkpoint['quarter'] = None\n",
        "    \n",
        "    # Create processed_combos if we have year and quarter\n",
        "    if df_checkpoint['year'].notna().any() and df_checkpoint['quarter'].notna().any():\n",
        "        df_checkpoint['year'] = df_checkpoint['year'].astype(int)\n",
        "        df_checkpoint['quarter'] = df_checkpoint['quarter'].astype(int)\n",
        "        processed_combos = set(\n",
        "            zip(df_checkpoint['ticker'], df_checkpoint['year'], df_checkpoint['quarter'])\n",
        "        )\n",
        "        print(f\"\\nAlready processed combinations: {len(processed_combos):,}\")\n",
        "    else:\n",
        "        processed_combos = set()\n",
        "        print(f\"\\nWarning: Could not determine processed combinations. Will reprocess all.\")\n",
        "    \n",
        "    # Store for use in next cell\n",
        "    transcripts_list = df_checkpoint.to_dict('records')\n",
        "    print(f\"\\n✓ Recovery complete! Ready to resume processing.\")\n",
        "    print(f\"  Current transcripts: {len(transcripts_list):,}\")\n",
        "    print(f\"  Processed combinations: {len(processed_combos):,}\")\n",
        "    \n",
        "else:\n",
        "    print(\"No checkpoint files found. Starting fresh.\")\n",
        "    transcripts_list = []\n",
        "    processed_combos = set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created DataFrame with 63509 transcripts\n",
            "\n",
            "Columns: ['ticker', 'symbol', 'year', 'quarter', 'date', 'text_raw', 'text_presentation', 'text_qa', 'text_full', 'source_vendor', 'call_id', 'call_date', 'fiscal_year', 'fiscal_quarter', 'len_presentation', 'len_qa', 'len_full']\n",
            "\n",
            "Date range: 2009-05-21 00:00:00 to 2025-04-15 00:00:00\n",
            "\n",
            "Sample:\n",
            "  ticker  call_date  fiscal_year  fiscal_quarter  len_full\n",
            "0      A 2010-02-12         2010               1     30481\n",
            "1      A 2010-05-18         2010               2     44386\n",
            "2      A 2011-02-14         2011               1     56947\n",
            "3      A 2011-05-13         2011               2     51672\n",
            "4      A 2011-08-15         2011               3     55869\n",
            "5      A 2011-11-15         2011               4     56508\n",
            "6      A 2012-02-15         2012               1     68830\n",
            "7      A 2012-05-14         2012               2     44642\n",
            "8      A 2012-08-15         2012               3     62928\n",
            "9      A 2012-11-19         2012               4     58603\n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame\n",
        "if transcripts_list:\n",
        "    df_transcripts = pd.DataFrame(transcripts_list)\n",
        "    \n",
        "    # Parse dates\n",
        "    df_transcripts['call_date'] = pd.to_datetime(df_transcripts['date'], errors='coerce')\n",
        "    df_transcripts['fiscal_year'] = df_transcripts['year']\n",
        "    df_transcripts['fiscal_quarter'] = df_transcripts['quarter'].astype(int)\n",
        "    \n",
        "    # Add text length metrics\n",
        "    df_transcripts['len_presentation'] = df_transcripts['text_presentation'].str.len()\n",
        "    df_transcripts['len_qa'] = df_transcripts['text_qa'].str.len()\n",
        "    df_transcripts['len_full'] = df_transcripts['text_full'].str.len()\n",
        "    \n",
        "    # Sort by ticker and date\n",
        "    df_transcripts = df_transcripts.sort_values(['ticker', 'call_date']).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Created DataFrame with {len(df_transcripts)} transcripts\")\n",
        "    print(f\"\\nColumns: {list(df_transcripts.columns)}\")\n",
        "    print(f\"\\nDate range: {df_transcripts['call_date'].min()} to {df_transcripts['call_date'].max()}\")\n",
        "    print(f\"\\nSample:\")\n",
        "    print(df_transcripts[['ticker', 'call_date', 'fiscal_year', 'fiscal_quarter', 'len_full']].head(10))\n",
        "else:\n",
        "    print(\"No transcripts fetched. Check API key and network connection.\")\n",
        "    df_transcripts = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved 63509 transcripts to: /Users/david/Desktop/MATH-GA 2707/Moving Target/data/intermediate/transcripts_clean.parquet\n",
            "\n",
            "Summary Statistics:\n",
            "  Unique tickers: 1475\n",
            "  Average transcript length: 41823 characters\n",
            "  Transcripts with Q&A section: 58057\n",
            "  Date range: 2009-05-21 00:00:00 to 2025-04-15 00:00:00\n"
          ]
        }
      ],
      "source": [
        "# Save to parquet\n",
        "if not df_transcripts.empty:\n",
        "    output_file = config['data']['transcripts_clean']\n",
        "    df_transcripts.to_parquet(output_file, index=False, engine='pyarrow')\n",
        "    print(f\"\\nSaved {len(df_transcripts)} transcripts to: {output_file}\")\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    print(f\"  Unique tickers: {df_transcripts['ticker'].nunique()}\")\n",
        "    print(f\"  Average transcript length: {df_transcripts['len_full'].mean():.0f} characters\")\n",
        "    print(f\"  Transcripts with Q&A section: {(df_transcripts['len_qa'] > 0).sum()}\")\n",
        "    print(f\"  Date range: {df_transcripts['call_date'].min()} to {df_transcripts['call_date'].max()}\")\n",
        "else:\n",
        "    print(\"No data to save\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
