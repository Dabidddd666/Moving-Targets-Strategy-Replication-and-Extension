{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 2: Identifier Mapping & Fiscal Quarter Alignment\n",
        "\n",
        "This notebook:\n",
        "1. Maps ticker symbols to permno (for CRSP returns) and gvkey (for Compustat)\n",
        "2. Aligns calls to fiscal quarters\n",
        "3. Ensures consistent firm identifiers across the pipeline\n",
        "\n",
        "**Note**: For a full implementation, you would need CRSP and Compustat databases. \n",
        "This notebook provides a framework that can be adapted when those databases are available.\n",
        "\n",
        "**Performance**: Uses Polars for faster loading and processing (10-100x faster than pandas for large datasets).\n",
        "If Polars is not installed, run: `pip install polars`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "USE_POLARS = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading transcripts with Polars (fast mode)...\n",
            "✓ Loaded 63,509 transcripts in 27.62 seconds\n",
            "Columns: ['ticker', 'symbol', 'year', 'quarter', 'date', 'text_raw', 'text_presentation', 'text_qa', 'text_full', 'source_vendor', 'call_id', 'call_date', 'fiscal_year', 'fiscal_quarter', 'len_presentation', 'len_qa', 'len_full']\n",
            "Memory usage: 7790.3 MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # Keep pandas for compatibility if needed\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Load config\n",
        "BASE_DIR = Path('/Users/david/Desktop/MATH-GA 2707/Moving Target')\n",
        "CONFIG_DIR = BASE_DIR / 'configs'\n",
        "INTERMEDIATE_DIR = BASE_DIR / 'data' / 'intermediate'\n",
        " \n",
        "with open(CONFIG_DIR / 'base.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "for key in config['data']:\n",
        "    config['data'][key] = Path(config['data'][key])\n",
        "\n",
        "# Load transcripts - use Polars for speed if available\n",
        "start_time = time.time()\n",
        "if USE_POLARS:\n",
        "    print(\"Loading transcripts with Polars (fast mode)...\")\n",
        "    # Use lazy evaluation for even better performance on large files\n",
        "    df_pl = pl.scan_parquet(config['data']['transcripts_clean']).collect()\n",
        "    # Or use eager loading: df_pl = pl.read_parquet(config['data']['transcripts_clean'])\n",
        "else:\n",
        "    print(\"Loading transcripts with pandas...\")\n",
        "    df_pl = pd.read_parquet(config['data']['transcripts_clean'])\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"✓ Loaded {len(df_pl):,} transcripts in {load_time:.2f} seconds\")\n",
        "print(f\"Columns: {list(df_pl.columns)}\")\n",
        "if USE_POLARS:\n",
        "    print(f\"Memory usage: {df_pl.estimated_size('mb'):.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Created firm identifiers in 0.05 seconds\n",
            "Unique firms: 1,475\n",
            "Unique firm-quarters: 63,509\n",
            "\n",
            "Sample:\n",
            "shape: (10, 6)\n",
            "┌────────┬─────────┬─────────────────────┬────────┬──────┬─────────────────┐\n",
            "│ ticker ┆ firm_id ┆ call_date           ┆ fyearq ┆ fqtr ┆ firm_quarter_id │\n",
            "│ ---    ┆ ---     ┆ ---                 ┆ ---    ┆ ---  ┆ ---             │\n",
            "│ str    ┆ str     ┆ datetime[ns]        ┆ i64    ┆ i32  ┆ str             │\n",
            "╞════════╪═════════╪═════════════════════╪════════╪══════╪═════════════════╡\n",
            "│ A      ┆ A       ┆ 2010-02-12 00:00:00 ┆ 2010   ┆ 1    ┆ A_2010_Q1       │\n",
            "│ A      ┆ A       ┆ 2010-05-18 00:00:00 ┆ 2010   ┆ 2    ┆ A_2010_Q2       │\n",
            "│ A      ┆ A       ┆ 2011-02-14 00:00:00 ┆ 2011   ┆ 1    ┆ A_2011_Q1       │\n",
            "│ A      ┆ A       ┆ 2011-05-13 00:00:00 ┆ 2011   ┆ 2    ┆ A_2011_Q2       │\n",
            "│ A      ┆ A       ┆ 2011-08-15 00:00:00 ┆ 2011   ┆ 3    ┆ A_2011_Q3       │\n",
            "│ A      ┆ A       ┆ 2011-11-15 00:00:00 ┆ 2011   ┆ 4    ┆ A_2011_Q4       │\n",
            "│ A      ┆ A       ┆ 2012-02-15 00:00:00 ┆ 2012   ┆ 1    ┆ A_2012_Q1       │\n",
            "│ A      ┆ A       ┆ 2012-05-14 00:00:00 ┆ 2012   ┆ 2    ┆ A_2012_Q2       │\n",
            "│ A      ┆ A       ┆ 2012-08-15 00:00:00 ┆ 2012   ┆ 3    ┆ A_2012_Q3       │\n",
            "│ A      ┆ A       ┆ 2012-11-19 00:00:00 ┆ 2012   ┆ 4    ┆ A_2012_Q4       │\n",
            "└────────┴─────────┴─────────────────────┴────────┴──────┴─────────────────┘\n"
          ]
        }
      ],
      "source": [
        "# For now, we'll use ticker as the primary identifier\n",
        "# In a full implementation, you would:\n",
        "# 1. Load CRSP stocknames file to map ticker -> permno\n",
        "# 2. Load Compustat company file to map ticker -> gvkey\n",
        "# 3. Handle ticker changes over time\n",
        "\n",
        "# Create firm identifiers - optimized for speed\n",
        "start_time = time.time()\n",
        "\n",
        "if USE_POLARS:\n",
        "    # Using Polars expressions for efficient column operations\n",
        "    df_pl = df_pl.with_columns([\n",
        "        pl.col('ticker').alias('firm_id'),  # Will be replaced with permno/gvkey in full implementation\n",
        "        pl.lit(None).cast(pl.Utf8).alias('permno'),  # Placeholder - would come from CRSP\n",
        "        pl.lit(None).cast(pl.Utf8).alias('gvkey'),   # Placeholder - would come from Compustat\n",
        "        pl.col('fiscal_year').alias('fyearq'),\n",
        "        pl.col('fiscal_quarter').cast(pl.Int32).alias('fqtr'),\n",
        "    ])\n",
        "    \n",
        "    # Create unique firm-quarter identifier (Polars is much faster for string operations)\n",
        "    df_pl = df_pl.with_columns([\n",
        "        (pl.col('firm_id').cast(pl.Utf8) + '_' + \n",
        "         pl.col('fyearq').cast(pl.Utf8) + '_Q' + \n",
        "         pl.col('fqtr').cast(pl.Utf8)).alias('firm_quarter_id')\n",
        "    ])\n",
        "    \n",
        "    print(f\"✓ Created firm identifiers in {time.time() - start_time:.2f} seconds\")\n",
        "    print(f\"Unique firms: {df_pl['firm_id'].n_unique():,}\")\n",
        "    print(f\"Unique firm-quarters: {df_pl['firm_quarter_id'].n_unique():,}\")\n",
        "    print(f\"\\nSample:\")\n",
        "    print(df_pl.select(['ticker', 'firm_id', 'call_date', 'fyearq', 'fqtr', 'firm_quarter_id']).head(10))\n",
        "else:\n",
        "    # Pandas fallback\n",
        "    df_pl['firm_id'] = df_pl['ticker']\n",
        "    df_pl['permno'] = None\n",
        "    df_pl['gvkey'] = None\n",
        "    df_pl['fyearq'] = df_pl['fiscal_year']\n",
        "    df_pl['fqtr'] = df_pl['fiscal_quarter'].astype(int)\n",
        "    df_pl['firm_quarter_id'] = df_pl['firm_id'].astype(str) + '_' + df_pl['fyearq'].astype(str) + '_Q' + df_pl['fqtr'].astype(str)\n",
        "    \n",
        "    print(f\"✓ Created firm identifiers in {time.time() - start_time:.2f} seconds\")\n",
        "    print(f\"Unique firms: {df_pl['firm_id'].nunique():,}\")\n",
        "    print(f\"Unique firm-quarters: {df_pl['firm_quarter_id'].nunique():,}\")\n",
        "    print(f\"\\nSample:\")\n",
        "    print(df_pl[['ticker', 'firm_id', 'call_date', 'fyearq', 'fqtr', 'firm_quarter_id']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deduplicating by firm-quarter...\n",
            "✓ Deduplication completed in 0.03 seconds\n",
            "After deduplication: 63,509 unique firm-quarters\n",
            "Removed 0 duplicate calls\n",
            "✓ Saved mapped transcripts in 19.03 seconds\n",
            "File: /Users/david/Desktop/MATH-GA 2707/Moving Target/data/intermediate/transcripts_clean.parquet\n"
          ]
        }
      ],
      "source": [
        "# Handle multiple calls per quarter\n",
        "# Keep the most recent call per firm-quarter (or main earnings call)\n",
        "start_time = time.time()\n",
        "print(\"Deduplicating by firm-quarter...\")\n",
        "initial_count = len(df_pl)\n",
        "\n",
        "if USE_POLARS:\n",
        "    # Polars is much faster for groupby operations\n",
        "    df_mapped = (df_pl\n",
        "        .sort(['firm_quarter_id', 'call_date'])\n",
        "        .group_by('firm_quarter_id', maintain_order=False)\n",
        "        .last()\n",
        "    )\n",
        "else:\n",
        "    # Pandas fallback\n",
        "    df_mapped = df_pl.sort_values(['firm_quarter_id', 'call_date']).groupby('firm_quarter_id').last().reset_index(drop=True)\n",
        "\n",
        "dedup_time = time.time() - start_time\n",
        "print(f\"✓ Deduplication completed in {dedup_time:.2f} seconds\")\n",
        "print(f\"After deduplication: {len(df_mapped):,} unique firm-quarters\")\n",
        "print(f\"Removed {initial_count - len(df_mapped):,} duplicate calls\")\n",
        "\n",
        "# Save mapped transcripts\n",
        "start_time = time.time()\n",
        "if USE_POLARS:\n",
        "    # Polars write_parquet is also faster\n",
        "    df_mapped.write_parquet(config['data']['transcripts_clean'], compression='snappy')\n",
        "else:\n",
        "    df_mapped.to_parquet(config['data']['transcripts_clean'], index=False, engine='pyarrow', compression='snappy')\n",
        "\n",
        "save_time = time.time() - start_time\n",
        "print(f\"✓ Saved mapped transcripts in {save_time:.2f} seconds\")\n",
        "print(f\"File: {config['data']['transcripts_clean']}\")\n",
        "\n",
        "# Convert to pandas if needed for downstream processing (optional)\n",
        "# if USE_POLARS:\n",
        "#     df = df_mapped.to_pandas()\n",
        "# Or keep as Polars for faster operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
